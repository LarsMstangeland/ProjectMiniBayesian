{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 3 Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Lars Magne Stangeland (Student ID: ) and Haavard Fossdal (Student ID: 3040928803)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "import math as Math\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twenty-Questions: Unconstrained Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "file_Path = '../data/unigram_freq.csv'\n",
    "df = pd.read_csv(file_Path, keep_default_na=False)\n",
    "count = 0\n",
    "\n",
    "\n",
    "# trim to first 3000 rows\n",
    "df = df.sort_values(by='count', ascending=False)\n",
    "df = df.head(3000)\n",
    "\n",
    "#Build the Huffman tree\n",
    "def get_huffman_code(df):\n",
    "\n",
    "    # Dictionary to store the Huffman tree\n",
    "    huffman_tree = {}\n",
    "\n",
    "    while len(df) > 1:\n",
    "\n",
    "        #first group the two least frequent word\n",
    "        word1 = df.iloc[-1]\n",
    "        word2 = df.iloc[-2]\n",
    "\n",
    "        #Get cumulative frequency of the two least frequent words\n",
    "        sum_freq = word1['count'] + word2['count']\n",
    "        \n",
    "        #Make a set of two least frequent words\n",
    "        LeastWordsSet = np.array([word1['word'], word2['word']])\n",
    "        LeastWordsSetWithFreqent = np.append(LeastWordsSet, sum_freq)\n",
    "\n",
    "        #Create Key\n",
    "        # Key1 = df.index[df['word'] == word1['word']].tolist()\n",
    "        # Key2 = df.index[df['word'] == word2['word']].tolist()\n",
    "        # UniqueKey = str(Key1[0]) +'_'+ str(Key2[0])\n",
    "        UniqueKey = f\"{word1['word']}_{word2['word']}\"\n",
    "\n",
    "        # Store parent-child relationship in Huffman tree\n",
    "        huffman_tree[UniqueKey] = {\"left\": word1['word'], \"right\": word2['word'], \"frequency\": sum_freq}\n",
    "\n",
    "   \n",
    "        #dictionary of the words\n",
    "        Set1 = {UniqueKey: sum_freq}\n",
    "\n",
    "        # Create a new DataFrame for the new row\n",
    "        new_row = pd.DataFrame({'word': [UniqueKey], 'count': [sum_freq]})\n",
    "        \n",
    "        #remove the last two words\n",
    "        df = df.drop(df.tail(2).index)\n",
    "\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "        # Sort the DataFrame by count\n",
    "        df = df.sort_values(by='count', ascending=False)\n",
    "\n",
    "        #print(f\"Merged {word1['word']} and {word2['word']} -> {UniqueKey} ({sum_freq})\")\n",
    "\n",
    "\n",
    "\n",
    "    # print(\"\\nFinal Huffman Tree Root Node:\")\n",
    "    # print(df.head())\n",
    "    # print(f\"Total Nodes in Final Tree: {len(df)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # print(df.head())\n",
    "    # print(len(df))\n",
    "\n",
    "    #Print the Huffman tree dictionary\n",
    "    # print(\"\\nHuffman Tree Structure:\")\n",
    "    # for key, value in huffman_tree.items():\n",
    "    #     print(f\"{key}: {value}\")\n",
    "\n",
    "    # print how many nodes are in the tree\n",
    "\n",
    "    return huffman_tree\n",
    "\n",
    "def make_huffman_code(huffman_tree, prefix, node, huffman_code):\n",
    "    if node not in huffman_tree:\n",
    "        huffman_code[node] = prefix\n",
    "        return\n",
    "\n",
    "    left = huffman_tree[node]['left']\n",
    "    right = huffman_tree[node]['right']\n",
    "\n",
    "    make_huffman_code(huffman_tree, prefix + \"0\", left, huffman_code)\n",
    "    make_huffman_code(huffman_tree, prefix + \"1\", right, huffman_code)\n",
    "\n",
    "    return huffman_code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Fano_prosedure(df = df.head(6)):\n",
    "    \"\"\"\n",
    "    Finds the best split for Fano code\n",
    "    \"\"\"\n",
    "    #sorting count\n",
    "\n",
    "    # Create a new column to store the Fano code\n",
    "    df['fano_code'] = \"\"\n",
    "\n",
    "    # Calculate the cumulative frequency\n",
    "    df['cumulative_freq'] = df['count'].cumsum()\n",
    "\n",
    "    # Calculate the total frequency\n",
    "    total_freq = df['count'].sum()\n",
    "\n",
    "\n",
    "    # Calculate the average frequency\n",
    "    df['Normalized_cf'] = df['cumulative_freq'] / total_freq\n",
    "\n",
    "    min_index = (df['Normalized_cf']-0.5).abs().idxmin()\n",
    "    # print(df)\n",
    "    # print(f\"min index:{min_index}\")\n",
    "    return min_index\n",
    "    \n",
    "\n",
    "fano_tree={}\n",
    "\n",
    "def Fano_code(df, prefix):\n",
    "   \n",
    "    df = df.sort_values(by='count', ascending=False)\n",
    "\n",
    "\n",
    "    #Split the dataframe\n",
    "    smallest_index = Fano_prosedure(df)\n",
    "\n",
    "    \n",
    "    if len(df) > 1:\n",
    "        left = df.iloc[:smallest_index+1].reset_index(drop=True)\n",
    "        right = df.iloc[smallest_index+1:].reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        word = df.iloc[0]['word']\n",
    "        fano_tree[word] = prefix\n",
    "        return \n",
    "\n",
    "    \n",
    "    Fano_code(left, prefix + \"0\")\n",
    "    Fano_code(right, prefix + \"1\")\n",
    "\n",
    "    \n",
    "    return fano_tree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_probability_for_word(word, df = df):\n",
    "    \"\"\"\n",
    "    Get the probability of a word in the DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    word = str(word)\n",
    "    word_row = df[df['word'] == word]\n",
    "    if len(word_row) == 0:\n",
    "        return 0\n",
    "    return word_row['count'].values[0] / df['count'].sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_probs = {word: get_probability_for_word(word) for word in df['word']}\n",
    "\n",
    "# print(word_probs)\n",
    "\n",
    "def get_steps_in_fano(word, fano_tree):\n",
    "    \"\"\"\n",
    "    Get the number of steps in Fano code for a word\n",
    "    \"\"\"\n",
    "    return len(fano_tree[word])\n",
    "\n",
    "\n",
    "def average_code_length_fano(fano_tree, word_probs):\n",
    "    \"\"\"\n",
    "    Calculate the average code length for Fano code\n",
    "    \"\"\"\n",
    "    return sum([get_steps_in_fano(word, fano_tree) * word_probs[word] for word in word_probs])\n",
    "\n",
    "\n",
    "\n",
    "def get_steps_in_huffman(word, huffman_tree):\n",
    "    \"\"\"\n",
    "    Get the number of steps in Huffman code for a word\n",
    "    \"\"\"\n",
    "    return len(huffman_tree[word])\n",
    "\n",
    "def average_code_length_huffman(huffman_tree, word_probs):\n",
    "    \"\"\"\n",
    "    Calculate the average code length for Huffman code\n",
    "    \"\"\"\n",
    "    return sum([get_steps_in_huffman(word, huffman_tree) * word_probs[word] for word in word_probs])\n",
    "\n",
    "huffman_tree = get_huffman_code(df)\n",
    "root_node = list(huffman_tree.keys())[-1]\n",
    "\n",
    "huffman_code = make_huffman_code(huffman_tree, \"\", root_node, {})\n",
    "\n",
    "fano_code = Fano_code(df, prefix='')\n",
    "\n",
    "\n",
    "Huffman_avg_steps = average_code_length_huffman(huffman_code, word_probs)\n",
    "Fano_avg_steps = average_code_length_fano(fano_code, word_probs)\n",
    "\n",
    "difference_in_approaches = abs(Fano_avg_steps - Huffman_avg_steps)\n",
    "\n",
    "print(f\"Average code length for Fano code: {Fano_avg_steps}\")\n",
    "print(f\"Average code length for Huffman code: {Huffman_avg_steps}\")\n",
    "print(f\"Difference in approaches: {difference_in_approaches}\")\n",
    "\n",
    "\n",
    "# compute shannon entropy\n",
    "\n",
    "def FindH(word_probs):\n",
    "    \"\"\"\n",
    "    Compute Shannon Entropy\n",
    "    \"\"\"\n",
    "    H = 0\n",
    "    for word in word_probs:\n",
    "        H += word_probs[word] * Math.log2(word_probs[word])\n",
    "    H = -H\n",
    "    return H\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "H = FindH(word_probs)\n",
    "print(f\"Shannon Entropy: {H}\")\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Average code length for Huffman code: {average_code_length_huffman(make_huffman_code(get_huffman_code()), word_probs)}\")\n",
    "\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average code length for Fano code: 9.689373530048671\n",
    "Average code length for Huffman code: 9.66819374072265\n",
    "Difference in approaches: 0.021179789326021492\n",
    "Shannon Entropy: 9.641114291987398\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twenty-Questions: Constrained Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code computes the word embeddings for the top 3000 words using the imported glove model. the distance measure is based on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "##### a)\n",
    "#######################################################################################################################\n",
    "\n",
    "# Load the data from the CSV file\n",
    "file_path = './data/unigram_freq.csv'\n",
    "df = pd.read_csv(file_path, keep_default_na=False)\n",
    "\n",
    "# Ttrim to the top 3000 most frequent words\n",
    "df = df.sort_values(by='count', ascending=False).head(3000)\n",
    "\n",
    "# Load the GloVe word embeddings model\n",
    "model = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "# dictionary to store the normalized embeddings for the words\n",
    "word_embeddings = {}\n",
    "missing_words = []\n",
    "\n",
    "# For each word in the DataFrame, we will attempt to get its embedding\n",
    "for word in df['word']:\n",
    "    if word in model:\n",
    "        embedding = model[word]\n",
    "        # Normalize the embedding to have unit norm (i.e. project onto the unit sphere)\n",
    "        norm = np.linalg.norm(embedding)\n",
    "        if norm != 0:\n",
    "            word_embeddings[word] = embedding / norm\n",
    "        else:\n",
    "            missing_words.append(word)\n",
    "    else:\n",
    "        missing_words.append(word)\n",
    "\n",
    "print(f\"Embeddings found for {len(word_embeddings)} out of {len(df)} words.\")\n",
    "print(f\"Example missing words: {missing_words[:10]}\")  # printing some of the missing words\n",
    "\n",
    "# we create a DataFrame that contains only the words with available embeddings\n",
    "df_embedded = df[df['word'].isin(word_embeddings.keys())].reset_index(drop=True)\n",
    "print(\"DataFrame of words with embeddings:\")\n",
    "print(df_embedded.head())\n",
    "\n",
    "# the word_embeddings dictionary has the normalized embeddings for the words, with df_embedded being the corresponding DataFrame.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the embedding model to project the top 3000 words onto the unit sphere by normalizing their embeddings. Words that cannot be embedded by our choice of embedding model are discarded. Similarly as previously done, the prior probabilities is calulated using the frequencies of the words in the list.\n",
    "\n",
    "We iteratively identify a hyperplane that divided the embedded words into two sets, and use this for the iterative procedure of finding the unknown word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################################################################################################\n",
    "##### b)\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Project words onto the unit sphere and compute priors\n",
    "# --------------------------------------------------------------------\n",
    "# df_embedded have the top 3,000 words with embeddings.\n",
    "# word_embeddings is a dictionary mapping each word to its normalized embedding.\n",
    "total_count = df_embedded['count'].sum()\n",
    "df_embedded = df_embedded.copy()  # create a local copy to avoid modifying the original\n",
    "df_embedded['prob'] = df_embedded['count'] / total_count  # weight words by their frequency (prior probability)\n",
    "word_probs = dict(zip(df_embedded['word'], df_embedded['prob']))\n",
    "\n",
    "# this function is used to sample a random unit vector (used for candidate hyperplanes)\n",
    "def sample_random_unit_vector(dim):\n",
    "    v = np.random.randn(dim)\n",
    "    return v / np.linalg.norm(v)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. Iteratively identify a hyperplane that splits the current probability mass aprpox. 50/50\n",
    "# --------------------------------------------------------------------\n",
    "def find_best_hyperplane(candidates, word_embeddings, word_probs, num_candidates=100):\n",
    "    dim = len(next(iter(word_embeddings.values())))\n",
    "    best_n = None\n",
    "    best_diff = float('inf')\n",
    "    for _ in range(num_candidates):\n",
    "        # this samples a random unit vector\n",
    "        n = sample_random_unit_vector(dim)\n",
    "        # here we compute the probability mass on the \"positive\" side (i.e. where dot product >= 0)\n",
    "        pos_mass = sum(word_probs[w] for w in candidates if np.dot(word_embeddings[w], n) >= 0)\n",
    "        diff = abs(pos_mass - 0.5)  # we want a split as close as possible to 50/50\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_n = n\n",
    "    return best_n\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3, 4, and 5. Ask the question, drop wrong-side words, and iterate until resolution\n",
    "# --------------------------------------------------------------------\n",
    "def simulate_constrained_question_procedure(df_embedded, word_embeddings, word_probs):\n",
    "    # Create the list of candidate words from our DataFrame (in 1. we already handled the projection and priors)\n",
    "    candidates = list(df_embedded['word'])\n",
    "    \n",
    "    # we then sample an unknown word from the candidates using the prior probabilities\n",
    "    words = np.array(candidates)\n",
    "    probs = np.array([word_probs[w] for w in candidates])\n",
    "    unknown = np.random.choice(words, p=probs)\n",
    "    print(\"Unknown word (drawn from prior):\", unknown)\n",
    "    \n",
    "    steps = 0\n",
    "    # Set a variable to track if we are not reducing the candidate set\n",
    "    stagnation_count = 0\n",
    "    # Define a threshold for how many iterations with no reduction we'll tolerate\n",
    "    STAGNATION_THRESHOLD = 5  # you can adjust this as needed\n",
    "    # we continue the process until we have only one candidate left\n",
    "    while len(candidates) > 1:\n",
    "        steps += 1\n",
    "        \n",
    "        # we find a hyperplane that best splits the probability mass among the current candidates\n",
    "        n = find_best_hyperplane(candidates, word_embeddings, word_probs, num_candidates=100)\n",
    "        \n",
    "        # we check if the unknown word is on the same side as the hyperplane\n",
    "        unknown_side = np.dot(word_embeddings[unknown], n) >= 0\n",
    "        \n",
    "        # we update the candidate list based on the side of the hyperplane\n",
    "        if unknown_side:\n",
    "            new_candidates = [w for w in candidates if np.dot(word_embeddings[w], n) >= 0]\n",
    "        else:\n",
    "            new_candidates = [w for w in candidates if np.dot(word_embeddings[w], n) < 0]\n",
    "        \n",
    "        # Debug output to show how the probability mass is split.\n",
    "        pos_mass = sum(word_probs[w] for w in candidates if np.dot(word_embeddings[w], n) >= 0)\n",
    "        neg_mass = 1 - pos_mass\n",
    "        print(f\"Step {steps}: {len(candidates)} candidates; split: {pos_mass:.3f} vs. {neg_mass:.3f}.\")\n",
    "\n",
    "\n",
    "        # we check if the candidate set has shrunk significantly.\n",
    "        if len(new_candidates) < len(candidates):\n",
    "            stagnation_count = 0  # Reset if we made progress\n",
    "        else:\n",
    "            stagnation_count += 1  # No reduction detected\n",
    "        \n",
    "        # Fallback: if we have several iterations without reduction, choose the highest-probability candidate\n",
    "        if stagnation_count >= STAGNATION_THRESHOLD:\n",
    "            print(\"Candidate set did not shrink after several iterations. Using fallback selection.\")\n",
    "            chosen_candidate = max(candidates, key=lambda w: word_probs[w])\n",
    "            print(\"Fallback candidate chosen:\", chosen_candidate)\n",
    "            return chosen_candidate\n",
    "        \n",
    "        # Updatate of the candidate list with the words that remain after dropping the wrong side.\n",
    "        candidates = new_candidates\n",
    "        print(f\"After step {steps}, {len(candidates)} candidates remain.\\n\")\n",
    "    \n",
    "    print(\"Final candidate:\", candidates[0])\n",
    "    print(\"Total questions asked:\", steps)\n",
    "\n",
    "\n",
    "#a run of the procedure\n",
    "# simulate_constrained_question_procedure(df_embedded, word_embeddings, word_probs)\n",
    "\n",
    "## comments: in some runs of the simulation, we get a heavy-tailed distributino of the word ferquencies.\n",
    "## therefore, we added a check in the iterative loop, such that if after an iteration the candidate set hasn’t reduced in size, we break the loop and choose the candidate with the highest prior probability.\n",
    "## The fallback mechanism is a way to handle cases where the hyperplane splits are not effective in reducing the candidate set.\n",
    "\n",
    "\n",
    "\n",
    "# Modified simulation function that returns the number of steps (questions) taken\n",
    "def simulate_constrained_question_procedure_return_steps(df_embedded, word_embeddings, word_probs, stagnation_threshold=5, num_candidates=100):\n",
    "    # Start with all candidate words\n",
    "    candidates = list(df_embedded['word'])\n",
    "    # Sample an unknown word using the prior probabilities\n",
    "    words = np.array(candidates)\n",
    "    probs = np.array([word_probs[w] for w in candidates])\n",
    "    unknown = np.random.choice(words, p=probs)\n",
    "    \n",
    "    steps = 0\n",
    "    stagnation_count = 0  # tracks consecutive iterations with no reduction in candidate count\n",
    "    \n",
    "    while len(candidates) > 1:\n",
    "        steps += 1\n",
    "        n = find_best_hyperplane(candidates, word_embeddings, word_probs, num_candidates=num_candidates)\n",
    "        # Determine the side of the hyperplane where the unknown word lies\n",
    "        unknown_side = np.dot(word_embeddings[unknown], n) >= 0\n",
    "        # Partition candidates based on the hyperplane's side\n",
    "        if unknown_side:\n",
    "            new_candidates = [w for w in candidates if np.dot(word_embeddings[w], n) >= 0]\n",
    "        else:\n",
    "            new_candidates = [w for w in candidates if np.dot(word_embeddings[w], n) < 0]\n",
    "        \n",
    "        # Check if the candidate set is reduced\n",
    "        if len(new_candidates) < len(candidates):\n",
    "            stagnation_count = 0\n",
    "        else:\n",
    "            stagnation_count += 1\n",
    "        \n",
    "        # Fallback: if no reduction for several iterations, select the highest-probability candidate\n",
    "        if stagnation_count >= stagnation_threshold:\n",
    "            chosen_candidate = max(candidates, key=lambda w: word_probs[w])\n",
    "            candidates = [chosen_candidate]\n",
    "            break\n",
    "        \n",
    "        candidates = new_candidates\n",
    "        \n",
    "    return steps\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We noticed after some runs of the procedure that the we got a heavy-tailed distribution of the word probabilities. Therefore, we added a check in the iterative loop, such that if after an iteration the candidate set hasn’t reduced in size, we break the loop and choose the candidate with the highest prior probability. The fallback mechanism is a way to handle cases where the hyperplane splits are not effective in reducing the candidate set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we simulate this procedure over multiple runs to estimate the expected number of questions one should ask to resolve the unknown word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################################################################################################\n",
    "##### c)\n",
    "#######################################################################################################################\n",
    "\n",
    "# Run the simulation multiple times to estimate the expected number of questions\n",
    "num_trials = 100  # we adjust this number as needed\n",
    "steps_list = []\n",
    "for i in range(num_trials):\n",
    "    steps = simulate_constrained_question_procedure_return_steps(df_embedded, word_embeddings, word_probs)\n",
    "    steps_list.append(steps)\n",
    "\n",
    "avg_steps = np.mean(steps_list)\n",
    "std_steps = np.std(steps_list)\n",
    "print(f\"Average number of questions over {num_trials} trials: {avg_steps:.2f} ± {std_steps:.2f}\")\n",
    "\n",
    "# Compute Shannon entropy of the prior distribution as a lower bound\n",
    "def shannon_entropy(word_probs):\n",
    "    H = 0\n",
    "    for p in word_probs.values():\n",
    "        if p > 0:\n",
    "            H -= p * np.log2(p)\n",
    "    return H\n",
    "\n",
    "H = shannon_entropy(word_probs)\n",
    "print(f\"Shannon entropy (lower bound on questions): {H:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We computed the Shannon entropy to be 9.64.\n",
    "\n",
    "After 10 simluation runs of the procedure, we found out that the constrained procedure is taking, on average, around 105 questions (with a high variance). This isexpected when using a restricted set of linear questions based on word embeddings, epsecially when the prior is heavily skewed. The optimal Shannon bound questions is only achievable if one designs an  ideal decision tree, which is not possible with the given constraints. The large gap between them indicated thhe cosntraiend procedure and the ideal procedure shows that the constrained procedure is much less efficient.  The high variance suggests that some trials resolve relatively quickly, while others stall, which is likely due to the heavy-tailed nature of the word frequencie.\n",
    "\n",
    "A potential scheme that could be better than our approach here is to use the covariance structure of the candidate embeddings to choose hyperplanes along the direction of the greatest variance, which could yield a more balanced split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman: Noisy Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
